=== SmallMind Model Creation Profiling ===

--- Tiny Model Creation ---
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
  Min:    2.97 ms
  Median: 2.99 ms
  Avg:    3.50 ms
  Max:    5.52 ms

--- Small Model Creation ---
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
  Min:    22.27 ms
  Median: 22.34 ms
  Avg:    22.33 ms
  Max:    22.38 ms

--- Medium Model Creation ---
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
  Min:    58.56 ms
  Median: 99.27 ms
  Avg:    89.22 ms
  Max:    112.60 ms


=== Profiling Complete ===
