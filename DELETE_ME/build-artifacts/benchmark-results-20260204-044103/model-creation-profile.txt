=== SmallMind Model Creation Profiling ===

--- Tiny Model Creation ---
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
  Min:    2.93 ms
  Median: 2.96 ms
  Avg:    3.45 ms
  Max:    5.44 ms

--- Small Model Creation ---
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
  Min:    22.04 ms
  Median: 22.19 ms
  Avg:    22.17 ms
  Max:    22.31 ms

--- Medium Model Creation ---
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
  Min:    49.40 ms
  Median: 61.78 ms
  Avg:    65.73 ms
  Max:    101.10 ms


=== Profiling Complete ===
