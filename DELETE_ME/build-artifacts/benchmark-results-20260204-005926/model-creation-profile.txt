=== SmallMind Model Creation Profiling ===

--- Tiny Model Creation ---
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
  Min:    2.94 ms
  Median: 2.99 ms
  Avg:    3.47 ms
  Max:    5.46 ms

--- Small Model Creation ---
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
  Min:    21.77 ms
  Median: 21.85 ms
  Avg:    21.86 ms
  Max:    21.95 ms

--- Medium Model Creation ---
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
  Min:    49.07 ms
  Median: 61.15 ms
  Avg:    66.39 ms
  Max:    100.13 ms


=== Profiling Complete ===
