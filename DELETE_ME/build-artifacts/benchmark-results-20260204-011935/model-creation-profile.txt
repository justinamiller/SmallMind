=== SmallMind Model Creation Profiling ===

--- Tiny Model Creation ---
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
TransformerModel initialized: vocab=50, block_size=64, n_embd=128, n_layer=2, n_head=4, dropout=0.1
Total parameters: 417,792 (29 tensors)
  Min:    2.96 ms
  Median: 3.09 ms
  Avg:    3.55 ms
  Max:    5.61 ms

--- Small Model Creation ---
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=256, n_layer=4, n_head=4, dropout=0.1
Total parameters: 3,243,520 (53 tensors)
  Min:    22.44 ms
  Median: 23.13 ms
  Avg:    24.16 ms
  Max:    27.76 ms

--- Medium Model Creation ---
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
TransformerModel initialized: vocab=100, block_size=128, n_embd=384, n_layer=6, n_head=6, dropout=0.1
Total parameters: 10,773,504 (77 tensors)
  Min:    50.21 ms
  Median: 63.41 ms
  Avg:    73.58 ms
  Max:    102.26 ms


=== Profiling Complete ===
