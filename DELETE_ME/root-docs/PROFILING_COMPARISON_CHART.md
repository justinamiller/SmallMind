# SmallMind Profiling Comparison Chart

**Runs Analyzed:**
1. **Run 1:** 2026-02-04 00:59:26 (Oldest)
2. **Run 2:** 2026-02-04 01:19:35 (Previous)
3. **Run 3:** 2026-02-04 02:02:13 (Current)

---

## üìä Overall Metrics Trend

### Total Runtime (milliseconds)

```
Run 1 (00:59):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (data not available)
Run 2 (01:19):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 5,928 ms
Run 3 (02:02):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 9,237 ms (+55.8%)
```

### Total Memory Allocations (megabytes)

```
Run 1 (00:59):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (data not available)
Run 2 (01:19):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 2,550 MB
Run 3 (02:02):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  339 MB (-86.7%)
```

---

## üî• Hot Path Comparison

### Model Medium Inference

| Run | Time (ms) | Allocations (MB) | Tokens/Sec |
|-----|-----------|-----------------|------------|
| Run 2 | 1,201 | 730 | 20.8 |
| Run 3 | 1,863 | 83 | 13.4 |
| **Change** | **+55%** ‚ö†Ô∏è | **-89%** ‚úÖ | **-36%** ‚ö†Ô∏è |

### Model Small Inference

| Run | Time (ms) | Allocations (MB) | Tokens/Sec |
|-----|-----------|-----------------|------------|
| Run 2 | 532 | 109 | 47.0 |
| Run 3 | 444 | 19 | 56.3 |
| **Change** | **-17%** ‚úÖ | **-83%** ‚úÖ | **+20%** ‚úÖ |

---

## ‚ö° SIMD Operations Trend

### MatMul 512√ó512

| Run | Time (ms) | GFLOPS | Status |
|-----|-----------|--------|--------|
| Run 2 | 172 | 1.56 | Baseline |
| Run 3 | 906 | 0.30 | **-81% GFLOPS** ‚ö†Ô∏è |

**Visualization:**
```
Run 2:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 172 ms (GFLOPS: 1.56)
Run 3:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 906 ms (GFLOPS: 0.30)
         ‚ö†Ô∏è 5.3√ó SLOWER
```

### MatMul 256√ó256

| Run | Time (ms) | GFLOPS | Status |
|-----|-----------|--------|--------|
| Run 2 | 19.6 | 1.73 | Baseline |
| Run 3 | 113 | 0.30 | **-83% GFLOPS** ‚ö†Ô∏è |

### MatMul 128√ó128

| Run | Time (ms) | GFLOPS | Status |
|-----|-----------|--------|--------|
| Run 2 | 3.5 | 1.19 | Baseline |
| Run 3 | 13.3 | 0.32 | **-73% GFLOPS** ‚ö†Ô∏è |

---

## üéØ Activation Functions Trend

### GELU Performance

| Size | Run 2 (ms) | Run 3 (ms) | Change |
|------|------------|------------|--------|
| 1K | 2.3 | 1.0 | **-55%** ‚úÖ |
| 10K | 1.2 | 2.3 | **+96%** ‚ö†Ô∏è |
| 100K | 11.1 | 20.2 | **+82%** ‚ö†Ô∏è |
| 1M | 100.6 | 202.4 | **+101%** ‚ö†Ô∏è |

**Pattern:** Improved on small sizes, regressed on large sizes (inflection point ~5K elements)

### Softmax Performance

| Size | Run 2 (ms) | Run 3 (ms) | Change |
|------|------------|------------|--------|
| 256 | 7.2 | 2.5 | **-66%** ‚úÖ |
| 512 | 0.06 | 0.07 | **+17%** ‚û°Ô∏è |
| 1024 | 0.15 | 0.15 | **0%** ‚û°Ô∏è |
| 2048 | 6.2 | 0.3 | **-96%** ‚úÖ |

**Pattern:** Consistently improved across all sizes

---

## üíæ Memory Optimization Impact

### Per-Operation Allocation Comparison

| Operation | Run 2 (MB) | Run 3 (MB) | Reduction |
|-----------|------------|------------|-----------|
| Medium Model Forward | 729.96 | 83.10 | **-88.6%** |
| Small Model Forward | 109.26 | 19.00 | **-82.6%** |
| TensorAdd | 0.38 | 0.38 | 0% |
| GELU 1M | 0.01 | 0.01 | 0% |
| MatMul | 0.00 | 0.00 | 0% |

**Analysis:** Memory reduction focused on model inference, not low-level ops.

---

## üìà Performance Trajectory

### What Improved (Run 2 ‚Üí Run 3)

1. **Memory Efficiency** ‚¨ÜÔ∏è‚¨ÜÔ∏è‚¨ÜÔ∏è
   - Massive 87% reduction in allocations
   - Model inference memory down 83-89%
   - TensorPool and in-place ops working

2. **Small Model Performance** ‚¨ÜÔ∏è
   - 17% faster inference
   - 20% higher throughput
   - 41% faster model creation

3. **Softmax Operations** ‚¨ÜÔ∏è‚¨ÜÔ∏è
   - 66-96% faster across sizes
   - Consistently improved

4. **Tensor Operations** ‚¨ÜÔ∏è
   - 65-79% faster adds/broadcasts
   - Effective in-place optimizations

### What Regressed (Run 2 ‚Üí Run 3)

1. **Matrix Multiplication** ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è
   - 275-477% slower
   - GFLOPS dropped 73-83%
   - Critical performance bottleneck

2. **Medium Model Performance** ‚¨áÔ∏è‚¨áÔ∏è
   - 55% slower inference
   - 36% lower throughput
   - Dominated by MatMul regression

3. **GELU (Large)** ‚¨áÔ∏è
   - 82-101% slower on 10K+ elements
   - Small sizes improved, large sizes regressed

4. **Overall Runtime** ‚¨áÔ∏è
   - 56% slower total
   - Not acceptable for production

---

## üéØ Performance Goals vs Actual

### Target Metrics (Industry Standard)

| Metric | Target | Run 2 | Run 3 | Status |
|--------|--------|-------|-------|--------|
| TTFT (P50) | <2 ms | 0 ms | 0 ms | ‚úÖ Excellent |
| Throughput | >50 tok/s | 47 tok/s | 56 tok/s | ‚úÖ Improved |
| Memory Efficiency | <100 MB | 2,550 MB | 339 MB | üü° Better but still high |
| MatMul GFLOPS | >1.0 | 1.56 | 0.30 | ‚ö†Ô∏è Regressed |

### vs Industry Leaders (CPU-only)

| Framework | Throughput | Memory/Token | SmallMind Run 3 |
|-----------|-----------|--------------|-----------------|
| llama.cpp | 50-200 tok/s | 1-5 MB | 56 tok/s, 0.76 MB |
| ONNX Runtime | 100-300 tok/s | 2-8 MB | 56 tok/s, 0.76 MB |
| Transformers.js | 10-50 tok/s | 10-30 MB | 56 tok/s, 0.76 MB |

**Verdict:** SmallMind Small model is competitive! Medium model needs MatMul fix to compete.

---

## üîç Deeper Analysis

### Why Did Memory Improve So Dramatically?

**Evidence from memory benchmarks:**

```
TensorPool:
  Without: 2.08 MB per 1000 iterations
  With:    0.12 MB per 1000 iterations
  Savings: 94.4%

In-Place Operations:
  Allocating: 2.09 MB
  In-Place:   0.04 MB
  Savings: 98.1%

Fused LayerNorm:
  Allocations: 0.70 KB (1000 iterations)
  Result: Zero-allocation ‚úÖ
```

**Techniques used:**
1. TensorPool for buffer reuse
2. In-place tensor operations
3. Fused kernel implementations
4. ArrayPool for temporary buffers

### Why Did MatMul Regress So Severely?

**Hypothesis:** Memory pooling overhead

**Evidence:**
- Regression scales with matrix size (larger = worse)
- Small matrices (64√ó64) barely affected (+4.5%)
- Large matrices (512√ó512) devastated (+426%)
- Memory reduction suggests aggressive pooling

**Likely culprit in code:**
- Pool allocate/deallocate in inner MatMul loops
- Cache misses from non-contiguous pooled memory
- SIMD broken by pooled array access patterns
- Bounds checking overhead in pooled arrays

**What to check:**
1. Look for `TensorPool.Rent/Return` calls in MatMul
2. Compare memory layout (contiguous vs pooled)
3. Check if SIMD intrinsics are still being used
4. Profile cache hit rates (L1/L2/L3)

---

## üí° Recommendations Summary

### üî¥ Critical Path to Production

1. **Fix MatMul regression** (Blocker)
   - Target: Get back to ~172ms for 512√ó512 (Run 2 level)
   - Approach: Remove pooling from MatMul hot path OR optimize pooled access
   - Success criteria: <5% regression vs Run 2

2. **Verify Medium model** (Blocker)
   - Target: <1,300ms inference (Run 2 was 1,201ms)
   - Depends on: MatMul fix
   - Success criteria: >20 tokens/sec

### üü° High Value Optimizations

3. **Port Softmax learnings**
   - Softmax improved 96%, understand why
   - Apply to other operations
   - Expected: 10-50% improvements elsewhere

4. **Fix large GELU**
   - 100K+ elements are 82-101% slower
   - May be same issue as MatMul
   - Lower priority than MatMul

### üü¢ Future Improvements

5. **Further memory optimization**
   - Already at 339 MB (down from 2,550 MB)
   - Target: <100 MB for production
   - Approach: More aggressive pooling in non-critical paths

6. **Parallel inference**
   - Current: Single-threaded
   - Opportunity: Multi-core batching
   - Expected: 2-4√ó throughput

---

## üìã Test Matrix for Next Run

After fixing MatMul, verify:

- [ ] MatMul 512√ó512 < 200ms (was 172ms in Run 2)
- [ ] Medium model < 1,300ms (was 1,201ms in Run 2)
- [ ] Memory still < 400 MB (currently 339 MB)
- [ ] Small model still < 500ms (currently 444ms)
- [ ] GELU 1M < 120ms (was 101ms in Run 2)
- [ ] Overall runtime < 7,000ms (was 5,928ms in Run 2)

**Target Grade: A**
- No regressions >10%
- Memory <400 MB maintained
- Ready for production

---

## üìä Raw Data Summary

### Run 2 (Previous - 2026-02-04 01:19:35)
```
Total Runtime:      5,927.59 ms
Total Allocations:  2,550.03 MB
Methods Profiled:   29
Top Operation:      Model_Medium_Inference (1,201 ms)
```

### Run 3 (Current - 2026-02-04 02:02:13)
```
Total Runtime:      9,237.19 ms (+55.8%)
Total Allocations:    338.62 MB (-86.7%)
Methods Profiled:   29
Top Operation:      Model_Medium_Inference (1,863 ms, +55.1%)
```

### Delta (Run 3 - Run 2)
```
Runtime:      +3,309.56 ms (+55.8%) ‚ö†Ô∏è
Allocations:  -2,211.42 MB (-86.7%) ‚úÖ
```

---

**Generated:** 2026-02-04 02:04 UTC  
**Comparison Tool:** SmallMind CodeProfiler
