name: Build and Test

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ main ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

# Prevent CI pileups. Keep only the latest run per branch/PR ref.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read

jobs:
  build-and-test:
    name: Build + Unit/Integration Tests (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    
    permissions:
      checks: write  # Required for EnricoMi/publish-unit-test-result-action
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      # Cache NuGet packages to speed up restore
      - name: Cache NuGet packages
        uses: actions/cache@v4
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-nuget-${{ hashFiles('**/*.csproj', '**/Directory.Build.props') }}
          restore-keys: |
            ${{ runner.os }}-nuget-

      - name: Restore dependencies
        run: dotnet restore SmallMind.sln

      - name: Build (Release)
        run: dotnet build SmallMind.sln --no-restore --configuration Release -maxcpucount

      - name: Run Unit Tests
        run: dotnet test tests/SmallMind.Tests/SmallMind.Tests.csproj --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=unit-tests.trx"

      - name: Run Regression Tests
        run: dotnet test tests/SmallMind.Tests/SmallMind.Tests.csproj --filter "Category=Regression" --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=regression-tests.trx"

      - name: Run Integration Tests
        run: dotnet test tests/SmallMind.IntegrationTests/SmallMind.IntegrationTests.csproj --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=integration-tests.trx"

      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            **/*.trx
          check_name: Test Results (${{ matrix.os }})
          comment_mode: off

  linux-perf-smoke:
    name: Linux Perf Smoke (nightly/manual/perf-label)
    runs-on: ubuntu-latest
    needs: build-and-test
    
    permissions:
      checks: write  # Required for EnricoMi/publish-unit-test-result-action

    # Run on schedule, workflow_dispatch, or PR with "perf" label
    if: |
      github.event_name == 'schedule' ||
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'pull_request' && contains(github.event.pull_request.labels.*.name, 'perf'))

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      # Cache NuGet packages to speed up restore (perf job)
      - name: Cache NuGet packages
        uses: actions/cache@v4
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-nuget-${{ hashFiles('**/*.csproj', '**/Directory.Build.props') }}
          restore-keys: |
            ${{ runner.os }}-nuget-

      - name: Restore dependencies
        run: dotnet restore SmallMind.sln

      - name: Build (Release)
        run: dotnet build SmallMind.sln --no-restore --configuration Release -maxcpucount

      - name: Run Performance Tests
        env:
          RUN_PERF_TESTS: 'true'
        run: dotnet test tests/SmallMind.PerfTests/SmallMind.PerfTests.csproj --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=perf-tests.trx"

      - name: Run Regression Benchmarks
        env:
          RUN_PERF_TESTS: 'true'
        run: dotnet test tests/SmallMind.PerfTests/SmallMind.PerfTests.csproj --filter "Category=Performance" --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=regression-benchmarks.trx"

      - name: Run SIMD Benchmarks
        run: |
          cd benchmarks
          dotnet run --configuration Release

      - name: Run SmallMind Performance Benchmarks
        continue-on-error: true
        run: |
          mkdir -p artifacts/perf
          cd src/SmallMind.Benchmarks
          dotnet run --configuration Release -- --output-dir ../../artifacts/perf --format json,markdown

      - name: Upload Perf Smoke Results (JSON + Markdown)
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: perf-smoke-results-${{ github.run_number }}
          path: |
            benchmarks/benchmark-results.md
            benchmarks/benchmark-results.json
            artifacts/perf/*.md
            artifacts/perf/*.json
          retention-days: 90

      - name: Check Performance Regressions
        continue-on-error: true
        run: |
          # Lenient regression check (start with 50% threshold)
          # Future: tighten thresholds and add baseline comparison
          echo "üîç Checking for performance regressions..."
          
          if [ -f "artifacts/perf/perf-results-latest.json" ]; then
            echo "‚úÖ Performance smoke benchmarks completed successfully"
            # TODO: Add jq-based regression detection comparing to baseline
            # Example: REGRESSION_THRESHOLD=1.5 (50% slowdown allowed initially)
          else
            echo "‚ö†Ô∏è Performance results not found - benchmark may have failed"
            echo "This is a soft failure (continue-on-error: true)"
          fi

      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            **/*.trx
          check_name: Perf Smoke Results
          comment_mode: off
