name: Build and Test

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ main ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'

# Prevent CI pileups. Keep only the latest run per branch/PR ref.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  checks: write
  contents: read
  pull-requests: write

jobs:
  build-and-test:
    name: Build + Unit/Integration Tests
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      # Cache NuGet packages to speed up restore
      - name: Cache NuGet packages
        uses: actions/cache@v4
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-nuget-${{ hashFiles('**/*.csproj', '**/Directory.Build.props') }}
          restore-keys: |
            ${{ runner.os }}-nuget-

      - name: Restore dependencies
        run: dotnet restore

      - name: Build (Release)
        run: dotnet build --no-restore --configuration Release -maxcpucount

      - name: Run Unit Tests
        run: dotnet test tests/SmallMind.Tests/SmallMind.Tests.csproj --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=unit-tests.trx"

      - name: Run Regression Tests
        run: dotnet test tests/SmallMind.Tests/SmallMind.Tests.csproj --filter "Category=Regression" --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=regression-tests.trx"

      - name: Run Integration Tests
        run: dotnet test tests/SmallMind.IntegrationTests/SmallMind.IntegrationTests.csproj --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=integration-tests.trx"

      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            **/*.trx
          check_name: Test Results
          comment_mode: off

  perf-and-benchmarks:
    name: Perf Tests + Benchmarks (nightly or PR label)
    runs-on: ubuntu-latest
    needs: build-and-test

    # Run on nightly schedule OR when PR has a "performance" label
    if: |
      github.event_name == 'schedule' ||
      (github.event_name == 'pull_request' && contains(toJson(github.event.pull_request.labels), '"name":"performance"'))

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'

      # Cache NuGet packages to speed up restore (perf job)
      - name: Cache NuGet packages
        uses: actions/cache@v4
        with:
          path: ~/.nuget/packages
          key: ${{ runner.os }}-nuget-${{ hashFiles('**/*.csproj', '**/Directory.Build.props') }}
          restore-keys: |
            ${{ runner.os }}-nuget-

      - name: Restore dependencies
        run: dotnet restore

      - name: Build (Release)
        run: dotnet build --no-restore --configuration Release -maxcpucount

      - name: Run Performance Tests
        env:
          RUN_PERF_TESTS: 'true'
        run: dotnet test tests/SmallMind.PerfTests/SmallMind.PerfTests.csproj --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=perf-tests.trx"

      - name: Run Regression Benchmarks
        env:
          RUN_PERF_TESTS: 'true'
        run: dotnet test tests/SmallMind.PerfTests/SmallMind.PerfTests.csproj --filter "Category=Performance" --configuration Release --no-build --verbosity normal --logger "trx;LogFileName=regression-benchmarks.trx"

      - name: Run SIMD Benchmarks
        run: |
          cd benchmarks
          dotnet run --configuration Release

      - name: Run SmallMind Performance Benchmarks
        run: |
          cd src/SmallMind.Benchmarks
          dotnet run --configuration Release

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmarks/benchmark-results.md
            benchmarks/benchmark-results.json
            artifacts/perf/perf-results-latest.md
            artifacts/perf/perf-results-latest.json
          retention-days: 30

      - name: Check Performance Regressions
        continue-on-error: true
        run: |
          # Future: Add baseline comparison and regression detection
          # For now, just ensure benchmarks ran successfully
          if [ -f "artifacts/perf/perf-results-latest.json" ]; then
            echo "✅ Performance benchmarks completed successfully"
          else
            echo "❌ Performance benchmarks failed to generate results"
            exit 1
          fi

      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            **/*.trx
          check_name: Perf & Benchmark Results
          comment_mode: off
