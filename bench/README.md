# SmallMind Benchmarking System

Real-model benchmarking suite for SmallMind CPU inference engine.

## Overview

This benchmarking system provides reproducible, CI-friendly performance measurements for SmallMind's CPU-based inference engine. It measures:

- **tok/s (tokens per second)**: Decode throughput during generation
- **TTFT (time-to-first-token)**: Latency from request start to first token
- **Peak RSS (memory)**: Maximum process working set during inference
- **Steady allocations**: Bytes allocated per token during decode phase
- **Thread scaling**: Performance across 1, 2, 4, 8, 16 threads
- **Context size scaling**: Performance at 256, 1k, 4k, 8k context sizes

## Quick Start

### Run CI Benchmarks (Fast)

```bash
cd /path/to/SmallMind
dotnet run -c Release --project bench/SmallMind.Benchmarks -- --ci-only
```

This runs only small models suitable for CI environments and automatically generates comparison reports with previous runs.

### Run Full Benchmarks

```bash
dotnet run -c Release --project bench/SmallMind.Benchmarks
```

Runs all models in the manifest (CI + optional larger models).

### Compare Results Across Runs and Architectures

```bash
# Generate comparison reports from existing results
dotnet run -c Release --project bench/SmallMind.Benchmarks -- compare

# Use custom results directory
dotnet run -c Release --project bench/SmallMind.Benchmarks -- compare \
    --results-dir /path/to/results

# Consolidate results into a single report
bash bench/consolidate-bench-results.sh bench/results
```

### Custom Configuration

```bash
# Specify context sizes and thread counts
dotnet run -c Release --project bench/SmallMind.Benchmarks -- \
    --contexts 256,1024,4096 \
    --threads 1,2,4,8 \
    --tokens 256 \
    --iterations 10

# Use custom manifest
dotnet run -c Release --project bench/SmallMind.Benchmarks -- \
    --manifest /path/to/custom-manifest.json

# Disable automatic comparison (only run benchmarks)
dotnet run -c Release --project bench/SmallMind.Benchmarks -- \
    --no-comparison
```

## Command-Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `--ci-only` | Run only CI models (small, fast) | false |
| `--manifest <path>` | Path to model manifest JSON | `bench/models/models.manifest.json` |
| `--contexts <list>` | Comma-separated context sizes | `256,1024` |
| `--threads <list>` | Comma-separated thread counts | `1,4` |
| `--tokens <n>` | Number of tokens to generate | `128` |
| `--iterations <n>` | Number of measurement iterations | `5` |
| `--no-comparison` | Disable automatic comparison with previous runs | false |

## Benchmark Comparison

The benchmarking system now includes automatic comparison capabilities to track performance changes over time and across architectures.

### Automatic Comparison

When you run benchmarks, the system automatically:
1. Searches for previous results matching the same model and architecture
2. Generates a comparison report showing performance changes
3. Creates a cross-architecture comparison if results from multiple architectures exist

### Comparison Reports

Three types of comparison reports are generated:

#### 1. Individual Comparison Reports
```
bench/results/<timestamp>_<model>_<arch>_comparison.md
```

Compares the current run with the most recent previous run for the same model and architecture. Shows:
- Time elapsed between runs
- Git commit differences
- Performance changes for each metric (TTFT, tok/s, memory, etc.)
- Visual indicators: ↑ (improvement), ↓ (regression), ≈ (no significant change)

**Example:**
```markdown
| Scenario | Metric | Current | Previous | Change |
|----------|--------|---------|----------|--------|
| ctx256_t1 | tok/s | 10.50 | 10.00 | ↑5.0% |
| ctx256_t1 | TTFT (ms) | 100.0 | 110.0 | ↑9.1% |
```

#### 2. Cross-Architecture Comparison
```
bench/results/<timestamp>_cross_architecture.md
```

Compares performance across all architectures (x64-linux, x64-windows, arm64-macos, etc.). Shows:
- Architecture details (CPU, cores, SIMD support)
- Side-by-side performance comparison
- Normalized efficiency metrics for fair comparison

**Example:**
```markdown
| Architecture | TTFT (ms) | tok/s | Peak RSS (MB) |
|--------------|-----------|-------|---------------|
| X64-Linux | 100.0 | 10.50 | 512.0 |
| ARM64-macOS | 120.0 | 8.75 | 480.0 |
```

#### 3. Consolidated Report
```
CONSOLIDATED_BENCHMARK_RESULTS.md
```

Generated by running `bash bench/consolidate-bench-results.sh`. Includes:
- All cross-architecture comparisons
- All individual comparison reports
- Latest benchmark results
- Summary statistics

### Comparison Metrics

The comparison system tracks changes in:
- **TTFT (Time to First Token)**: Lower is better
- **tok/s (Tokens per Second)**: Higher is better
- **tok/s E2E (End-to-End)**: Higher is better
- **Peak RSS (Memory)**: Lower is better
- **Alloc/tok (Allocations per Token)**: Lower is better

### Interpreting Changes

- **↑ Improvement**: Metric got better (higher throughput or lower latency/memory)
- **↓ Regression**: Metric got worse
- **≈ No Change**: Less than 1% difference (within noise threshold)

### Standalone Comparison Tool

You can also run comparisons separately:

```bash
# Compare all results in the default directory
dotnet run -c Release --project bench/SmallMind.Benchmarks -- compare

# Use custom results directory
dotnet run -c Release --project bench/SmallMind.Benchmarks -- compare \
    --results-dir /path/to/results

# Generate consolidated report
bash bench/consolidate-bench-results.sh /path/to/results
```

## Output Formats

Benchmarks produce multiple output formats per model:

### JSON (Machine-Readable)
```
bench/results/<timestamp>_<gitsha>_<os>_<model>.json
```
Complete results with full statistics and environment metadata.

### Markdown (README-Friendly)
```
bench/results/<timestamp>_<gitsha>_<os>_<model>.md
```
Human-readable summary tables suitable for documentation.

### CSV (Charting)
```
bench/results/<timestamp>_<gitsha>_<os>_<model>.csv
```
Tabular data for Excel, plotting tools, or time-series analysis.

### Comparison Reports (New)
```
bench/results/<timestamp>_<gitsha>_<os>_<model>_comparison.md
bench/results/<timestamp>_cross_architecture.md
```
Performance comparison vs. previous runs and across architectures.

## Model Manifest

Models are defined in `bench/models/models.manifest.json`:

```json
{
  "version": "1.0",
  "models": [
    {
      "name": "TinyStories-1M-Q4_0",
      "url": "https://example.com/model.gguf",
      "sha256": "abc123...",
      "size": 8388608,
      "quantType": "Q4_0",
      "contextLength": 512,
      "ci": true,
      "description": "Small model for CI",
      "tags": ["ci", "tiny"]
    }
  ]
}
```

### Adding a New Model

1. Add entry to manifest with:
   - `name`: Unique identifier
   - `url`: Public download URL (preferably HuggingFace)
   - `sha256`: SHA256 checksum for verification
   - `size`: File size in bytes
   - `quantType`: Quantization format (Q4_0, Q8_0, F16, etc.)
   - `contextLength`: Maximum context length
   - `ci`: true for fast CI models, false for manual-only

2. Run benchmarks - models are auto-downloaded and cached

## Model Caching

Models are cached to avoid re-downloading:

**Default cache**: `/tmp/SmallMind/BenchCache/`

**Custom cache**:
```bash
export SMALLMIND_BENCH_MODEL_CACHE=/path/to/cache
dotnet run -c Release --project bench/SmallMind.Benchmarks
```

Models are verified with SHA256 checksums. Invalid cached files are re-downloaded.

## Normalized Efficiency Metrics

The benchmarking system provides "Single CPU Unit" normalization for cross-machine comparison:

### Metrics

- **tok/s per core**: `tokensPerSecond / threadCount`
  - Isolates per-thread efficiency
  - 1-thread run = single-core performance
  - Multi-thread = average throughput per utilized core

- **tok/s per GHz per core**: `tokensPerSecond / (threadCount * cpuFrequencyGHz)`
  - Normalizes for CPU frequency differences
  - Requires CPU frequency detection (best-effort on Linux/macOS/Windows)
  
- **Cycles per token**: `(cpuFrequencyGHz * 1e9) / tokensPerSecond` (1-thread only)
  - Estimates CPU cycles consumed per token
  - Lower is better

- **Alloc/tok per GHz**: `allocBytesPerToken / cpuFrequencyGHz`
  - Memory efficiency normalized for frequency
  - Lower is better

### Interpretation

**These metrics emphasize implementation efficiency, not raw hardware power.**

- Compare implementations on different architectures (x64 vs ARM64)
- Compare compiler optimizations (.NET 9 vs .NET 10)
- Compare algorithm changes (with/without SIMD)
- Identify memory bottlenecks independent of CPU speed

**Limitations**:
- CPU frequency may not be detectable on all systems (reported as N/A)
- Turbo boost complicates effective frequency
- Cross-architecture comparison has inherent limits (x86 vs ARM instruction sets)

## Environment Metadata

All benchmark runs capture:

- Git commit SHA (for reproducibility)
- .NET runtime version
- OS description and architecture
- CPU model and core count
- CPU base/max frequency (if available)
- SIMD instruction support (SSE2, AVX, AVX2, AVX512, AdvSimd)
- GC mode (Server vs Workstation)

This metadata is included in all output formats.

## CI Integration

See `.github/workflows/bench-ci.yml` for automated multi-architecture benchmarking with comparison.

### CI Workflow Features

The CI benchmark workflow:
- **Runs on multiple architectures**: x64-linux, x64-windows, x64-macos, arm64-macos
- **Downloads previous results**: Automatically retrieves results from the last run for comparison
- **Generates comparison reports**: Shows performance changes vs. previous run
- **Creates cross-architecture comparison**: Consolidates results from all architectures
- **Displays results in GitHub UI**: Shows summary tables in the workflow summary
- **Uploads artifacts**: Stores results for 30 days (consolidated results for 90 days)

### CI Jobs

1. **benchmark** (matrix job): Runs on each architecture
   - Downloads previous results for that architecture
   - Runs CI benchmarks (`--ci-only`)
   - Generates comparison report
   - Uploads results as artifacts

2. **consolidate**: Runs after all benchmark jobs complete
   - Downloads results from all architectures
   - Generates cross-architecture comparison
   - Creates consolidated report
   - Uploads unified artifact

### Viewing CI Results

After a CI run completes:

1. **GitHub Actions Summary**: Click on a workflow run to see the summary page with benchmark tables
2. **Artifacts**: Download `benchmark-results-consolidated` for all results and comparisons
3. **Per-Architecture**: Download `benchmark-results-<arch>` for individual architecture results

### Running CI Benchmarks Locally

```bash
# Run the same benchmarks as CI
dotnet run -c Release --project bench/SmallMind.Benchmarks -- \
    --ci-only \
    --contexts 256,512,1024 \
    --threads 1,2,4 \
    --tokens 128 \
    --iterations 5
```

## Merge Results (Future)

```bash
dotnet run --project bench/SmallMind.Benchmarks -- merge \
    --input results/run1.json \
    --input results/run2.json \
    --output merged.md
```

(Not yet implemented)

## Architecture

### Projects

- **SmallMind.Benchmarks.Core**: Measurement engine, formatters, model downloader
- **SmallMind.Benchmarks**: Console application

### Key Classes

- `ModelManifest`: Model registry with SHA256 verification
- `ModelDownloader`: HTTP download with caching and checksum
- `BenchmarkHarness`: Iteration, warmup, statistics aggregation
- `EnvironmentInfo`: System metadata capture
- `OutputFormatter`: JSON/Markdown/CSV writers
- `NormalizationCalculator`: Cross-machine efficiency metrics
- `BenchmarkComparer`: Performance comparison and trend analysis (new)

## Measurement Methodology

1. **Warmup**: 1 warmup run (excluded from results)
2. **Iterations**: 5 measurement runs (configurable)
3. **Statistics**: Median, P90, Mean, Stddev reported
4. **GC**: Forced GC between warmup and measurements
5. **Memory**: Process peak RSS + per-thread allocation tracking
6. **Determinism**: Fixed seed (42) for reproducibility

## Contributing

When adding benchmarking features:

1. **No third-party libraries** (pure .NET/BCL only)
2. **Prefer Span<T>/Memory<T>** over allocations
3. **Cross-platform** (Linux, macOS, Windows; x64, ARM64)
4. **Deterministic** (fixed seeds, stable prompts)
5. **Versioned** (include git SHA in outputs)

## Examples

### Example Markdown Output

```markdown
## Performance Results

| Scenario | Context | Threads | tok/s | TTFT (ms) | Peak RSS (MB) |
|----------|---------|---------|-------|-----------|---------------|
| ctx256_t1 | 256 | 1 | 8.00 | 125.6 | 512.0 |
| ctx256_t4 | 256 | 4 | 32.00 | 125.6 | 512.0 |

## Normalized Efficiency Metrics

| Scenario | Threads | tok/s per core | Cycles/tok |
|----------|---------|----------------|------------|
| ctx256_t1 | 1 | 8.00 | 312500000 |
| ctx256_t4 | 4 | 8.00 | N/A |
```

## Troubleshooting

**Model download fails**:
- Check network connectivity
- Verify URL is accessible
- Confirm SHA256 matches (update manifest if model changed)

**No CPU frequency detected**:
- Normal on some cloud VMs
- Normalized metrics will show N/A
- Raw tok/s and TTFT still valid

**Out of memory**:
- Use smaller models or lower context sizes
- Check `--contexts` and `--tokens` parameters

## License

Same as SmallMind project (see repository root LICENSE).
